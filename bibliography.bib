@article{Beauchamp2017,
abstract = {Presidential, gubernatorial, and senatorial elections all require state-level polling, but con- tinuous real-time polling of every state during a campaign remains prohibitively expensive, and quite neglected for less competitive states. This paper employs a new dataset of over 500GB of politics-related Tweets from the final months of the 2012 presidential campaign to interpolate and predict state-level polling at the daily level. By modeling the correlations between existing state-level polls and the textual content of state-located Twitter data using a new combination of time-series cross-sectional methods plus bayesian shrinkage and model averaging, it is shown through forward-in-time out-of-sample testing that the textual con- tent of Twitter data can predict changes in fully representative opinion polls with a precision currently unfeasible with existing polling data. This could potentially allow us to estimate polling not just in less-polled states, but in unpolled states, in sub-state regions, and even on time-scaled shorter than a day, given the immense density of Twitter usage. Substantively, we can also examine the words most associated with changes in vote intention to discern the rich psychology and speech associated with a rapidly shifting national campaign.},
author = {Beauchamp, Nicholas},
doi = {10.1111/ajps.12274},
file = {:home/tom/Documents/Mendeley Desktop/Beauchamp - 2017 - Predicting and Interpolating State-level Polling using Twitter Textual Data.pdf:pdf},
issn = {00925853},
journal = {American Journal of Political Science},
mendeley-groups = {Articles/Text Analysis,Articles,Articles/Social Media,PhD,Negation paper},
number = {2},
pages = {490--503},
title = {{Predicting and Interpolating State-level Polling using Twitter Textual Data}},
volume = {61},
year = {2017}
}
@article{Benoit2016,
abstract = {Empirical social science often relies on data that are not observed in the field, but are transformed into quantitative variables by expert researchers who analyze and interpret qualitative raw sources. While generally considered the most valid way to produce data, this expert-driven process is inherently difficult to replicate or to assess on grounds of reliability. Using crowd-sourcing to distribute text for reading and interpretation by massive numbers of nonexperts, we generate results comparable to those using experts to read and interpret the same texts, but do so far more quickly and flexibly. Crucially, the data we collect can be reproduced and extended transparently, making crowd-sourced datasets intrinsically reproducible. This focuses researchers' attention on the fundamental scientific objective of specifying reliable and replicable methods for collecting the data needed, rather than on the content of any particular dataset. We also show that our approach works straightforwardly with different types of political text, written in different languages. While findings reported here concern text analysis, they have far-reaching implications for expert-generated data in the social sciences.},
author = {Benoit, Kenneth and Conway, Drew and Lauderdale, Benjamin E and Laver, Michael and Mikhaylov, Slava},
doi = {10.1017/S0003055416000058},
file = {:home/tom/Documents/Mendeley Desktop/Benoit et al. - 2016 - Crowd-sourced Text Analysis Reproducible and Agile Production of Political Data.pdf:pdf;:home/tom/Documents/Mendeley Desktop/Benoit et al. - 2016 - Crowd-sourced Text Analysis Reproducible and Agile Production of Political Data(2).pdf:pdf},
isbn = {0003055416000},
issn = {0003-0554},
journal = {American Political Science Review},
mendeley-groups = {Articles/Text Analysis,PhD,Negation paper},
number = {2},
pages = {278--295},
pmid = {1809027911},
title = {{Crowd-sourced Text Analysis: Reproducible and Agile Production of Political Data}},
volume = {110},
year = {2016}
}
@book{Berelson1952,
address = {New York},
author = {Berelson, B},
mendeley-groups = {Articles/Text Analysis,PhD,Negation paper},
publisher = {Free Press},
title = {{Content Analysis in Communications Research}},
year = {1952}
}
@book{Berelson1948,
address = {Chicago},
author = {Berelson, Bernard and Lazarsfeld, Paul F},
mendeley-groups = {Articles/Text Analysis,Negation paper},
publisher = {University of Chicago Press},
title = {{The Analysis of Communication Content}},
year = {1948}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/tom/Documents/Mendeley Desktop/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation(2).pdf:pdf},
isbn = {9781577352815},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {Artificial intelligence,Computing methodologies,Language resources,Learning settings,Machine learning,Machine learning approaches,Natural language processing,Neural networks},
mendeley-groups = {Articles,Articles/MY459 Quantitative Text Analysis,Articles/PhD/Text Analysis,Articles/Text Analysis,Articles/Text Analysis/Topic Modelling,PhD,Negation paper},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
url = {http://www.crossref.org/deleted{\_}DOI.html},
volume = {3},
year = {2003}
}
@book{Budge1987,
address = {Cambridge},
author = {Budge, Ian and Robertson, David and Hearl, Derek},
doi = {10.1017/S0143814X00004517},
editor = {Budge, Ian and Robertson, David and Hearl, Derek},
file = {:home/tom/Documents/Mendeley Desktop/Budge, Robertson, Hearl - 1987 - Ideology, Strategy, and Party Change Spatial Analyses of Post-War Election Programmes in 19 Democracies.pdf:pdf},
isbn = {9780521306485},
issn = {0143-814X},
mendeley-groups = {Articles/PhD/Text Analysis,PhD,Negation paper},
pmid = {530},
publisher = {Cambridge University Press},
title = {{Ideology, Strategy, and Party Change: Spatial Analyses of Post-War Election Programmes in 19 Democracies}},
year = {1987}
}
@book{Chomsky1957,
address = {The Hague},
author = {Chomsky, Noam},
file = {:home/tom/Documents/Mendeley Desktop/Chomsky - 1957 - Syntactic Stucture.pdf:pdf},
mendeley-groups = {Articles/NLP,PhD,Negation paper},
publisher = {Mouton {\&} Co},
title = {{Syntactic Stucture}},
year = {1957}
}
@article{Daubler2012,
abstract = {Our objective in this Research Note is to question the dominant approach to unitizing political texts prior to human coding. This is to parse texts into quasi-sentences (QSs), where a QS is defined as part or all of a natural sentence that states a distinct policy proposition. The use of the QS rather than a natural language unit (such as a sentence defined by punctuation) is motivated by the desire to capture all relevant political information, regardless of the stylistic decisions made by the author, for example, to use long or short natural sentences. The identification of QSs by human coders, however, is highly unreliable. If, comparing codings of the same texts using quasi-sentences and natural sentences, there is no appreciable difference in measured political content, then there is a strong case for replacing ‘endogenous' human unitization with ‘exogenous' unitization based on natural sentences that can be identified with perfect reliability by machines using pre-specified punctuation delimiters.},
author = {D{\"{a}}ubler, Thomas and Benoit, Kenneth and Mikhaylov, Slava and Laver, Michael},
doi = {10.1017/S0007123412000105},
file = {:home/tom/Documents/Mendeley Desktop/D{\"{a}}ubler et al. - 2012 - Natural Sentences as Valid Units for Coded Political Texts.pdf:pdf},
isbn = {0007123412000},
issn = {0007-1234},
journal = {British Journal of Political Science},
keywords = {Q Science (General)},
mendeley-groups = {Articles/MY459 Quantitative Text Analysis,Articles/PhD/Text Analysis,Articles/Text Analysis,PhD,Negation paper},
number = {4},
pages = {937--951},
title = {{Natural Sentences as Valid Units for Coded Political Texts}},
url = {http://journals.cambridge.org/action/displayJournal?jid=JPS},
volume = {42},
year = {2012}
}
@article{Dolezal2016,
abstract = {This article examines aspects of election manifestos that are largely ignored by extant manifesto-based studies focusing on issue saliencies and policy positions. Drawing on the literatures on negative campaigning, retrospective voting, party mandates and personalization, we develop a scheme of categories that allows for the analysis of attacks on competitors, references to a partys track record, subjective and objective policy pledges and the prominence of party leaders in manifestos. We also show that these elements are present in manifestos of major European parties. The relevance of these categories, we argue, should be influenced by a partys status in government or opposition, its ideology, its size, the relative popularity of party leaders and the occurrence of early elections. Our systematic examination of 46 Austrian election manifestos produced between 1986 and 2013 demonstrates that many of these expectations are supported by the evidence. Most notably, it emerges that government and opposition parties write manifestos that differ with respect to all of the five characteristics analysed. This suggests that there are systematic differences between government and opposition party manifestos that should be taken into consideration by scholars engaged in manifesto-based research.},
author = {Dolezal, Martin and Ennser-Jedenastik, Laurenz and Muller, Wolfgang C. and Praprotnik, Katrin and Winkler, Anna Katharina},
doi = {10.1177/1354068816678893},
file = {:home/tom/Documents/Mendeley Desktop/Dolezal et al. - 2016 - Beyond salience and position taking How political parties communicate through their manifestos.pdf:pdf},
issn = {1354-0688},
journal = {Party Politics},
keywords = {government,manifestos,negative campaigning,opposition,party track records,personalization,policy pledges},
mendeley-groups = {Articles/PS - Parties,PhD,Negation paper},
number = {June},
pages = {1--13},
title = {{Beyond salience and position taking: How political parties communicate through their manifestos}},
url = {http://ppq.sagepub.com/cgi/doi/10.1177/1354068816678893},
year = {2016}
}
@article{Dolezal2016a,
abstract = {We present a new method to analyze party manifestos to benefit the placement of political parties per se and to advance the study of elections. Our method improves on existing manual coding approaches by (1) generating semantically complete units based on syntax, (2) standardizing units into a subject–predicate–object structure, and (3) employing a fine-grained and flexible hierarchical coding scheme. We evaluate our approach by comparing estimates for the 2002, 2006, and 2008 Austrian national elections with those yielded by previous studies that employ the entire range of available measurement strategies. We also demonstrate how we link our new manifesto data with other kind of data produced in theAustrian National Election Study, especially mass and elite (party candidate) surveys.},
author = {Dolezal, Martin and Ennser-Jedenastik, Laurenz and M{\"{u}}ller, Wolfgang C. and Winkler, Anna Katharina},
doi = {10.1017/psrm.2015.38},
file = {:home/tom/Documents/Mendeley Desktop/Dolezal et al. - 2016 - Analyzing Manifestos in their Electoral Context A New Approach Applied to Austria, 2002–2008.pdf:pdf;:home/tom/Documents/Mendeley Desktop/Dolezal et al. - 2016 - Analyzing Manifestos in their Electoral Context A New Approach Applied to Austria, 2002–2008(2).pdf:pdf},
issn = {2049-8489},
journal = {Political Science Research and Methods},
mendeley-groups = {Articles/PS - Methodology,Articles/Text Analysis,PhD,Negation paper},
number = {3},
pages = {641--650},
title = {{Analyzing Manifestos in their Electoral Context A New Approach Applied to Austria, 2002–2008}},
url = {http://journals.cambridge.org/article{\_}S2049847015000382{\%}5Cnhttp://journals.cambridge.org/action/displayAbstract?fromPage=online{\&}aid=9925470{\&}fulltextType=RA{\&}fileId=S2049847015000382{\%}5Cnhttp://journals.cambridge.org/action/displayFulltext?type=1{\&}fid=9925495},
volume = {4},
year = {2016}
}
@book{Dryzek2004,
address = {Oxford},
author = {Dryzek, John},
mendeley-groups = {Articles/PS - Deliberation,Negation paper},
publisher = {Oxford University Press},
title = {{Deliberative Democracy and Beyond}},
year = {2004}
}
@article{Fishkin2005,
abstract = {The values of deliberation and political equality have proven hard to achieve simultaneously. Deliberative polling, which embodies both, provides a useful window on deliberative democracy. The results, responding to 'defeatist,' 'extenuationist,' and 'alarmist' critiques, sho that ordinary people can deliberate, that they benefit from doing so, and that the process neither biases nor polrizes their opinion.},
author = {Fishkin, James S and Luskin, Robert C},
doi = {10.1057/palgrave.ap.5500121},
file = {:home/tom/Documents/Mendeley Desktop/Fishkin, Luskin - 2005 - Experimenting with a democratic ideal Deliberative polling and public opinion.pdf:pdf},
isbn = {0001-6810},
issn = {0001-6810},
journal = {Acta Politica},
keywords = {deliberative democracy,deliberative polling,informed opinion,political equality},
mendeley-groups = {Articles/PS - Deliberation,Negation paper},
number = {3},
pages = {284--298},
title = {{Experimenting with a democratic ideal: Deliberative polling and public opinion}},
url = {http://www.palgrave-journals.com/doifinder/10.1057/palgrave.ap.5500121},
volume = {40},
year = {2005}
}
@article{Fox2005,
abstract = {A structural multilevel model is presented where some of the variables cannot be observed directly but are measured using tests or questionnaires. Observed dichotomous or ordinal polytomous response data serve to measure the latent variables using an item response theory model. The latent variables can be defined at any level of the multilevel model. A Bayesian procedure Markov chain Monte Carlo (MCMC), to estimate all parameters simultaneously is presented. It is shown that certain model checks and model comparisons can be done using the MCMC output. The techniques are illustrated using a simulation study and an application involving students' achievements on a mathematics test and test results regarding management characteristics of teachers and principles.},
author = {Fox, Jean-Paul},
doi = {10.1348/000711005X38951},
file = {:home/tom/Documents/Mendeley Desktop/Fox - 2005 - Multilevel IRT using dichotomous and polytomous response data.pdf:pdf},
isbn = {00071102},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
mendeley-groups = {Articles/Psychology - Methodology,Negation paper},
number = {1},
pages = {145--172},
pmid = {15969844},
title = {{Multilevel IRT using dichotomous and polytomous response data}},
volume = {58},
year = {2005}
}
@article{Fox2001,
abstract = {In this article, a two-level regression model is imposed on the ability parameters in an item response theory (IRT) model. The advantage of using latent rather than observed scores as dependent variables of a multilevel model is that it offers the possibility of separating the influence of item difficulty and ability level and modeling response variation and measurement error. Another advantage is that, contrary to observed scores, latent scores are test-independent, which offers the possibility of using results from different tests in one analysis where the parameters of the IRT model and the multilevel model can be concurrently estimated. The two-parameter normal ogive model is used for the IRT measurement model. It will be shown that the parameters of the two-parameter normal ogive model and the multilevel model can be estimated in a Bayesian framework using Gibbs sampling. Examples using simulated and real data are given.},
author = {Fox, Jean-Paul and Glas, Cees A.W.},
doi = {10.1007/BF02294839},
file = {:home/tom/Documents/Mendeley Desktop/Fox, Glas - 2001 - Bayesian Estimation of a Multilevel IRT Model Using Gibbs Sampling.pdf:pdf},
isbn = {0033-3123},
issn = {0033-3123},
journal = {Psychometrika},
keywords = {bayes estimates,gibbs sampler,irt,item response theory,markov chain monte carlo,multilevel model,two-parameter normal ogive model},
mendeley-groups = {Articles/Psychology - Methodology,Negation paper},
number = {2},
pages = {271--288},
title = {{Bayesian Estimation of a Multilevel IRT Model Using Gibbs Sampling}},
volume = {66},
year = {2001}
}
@article{Gough1965,
abstract = {This study tested an hypothesis that the hearer of a complex sentence must transform that sentence into the underlying kernel sentence before understanding it, and hence that speed of understanding a sentence would vary with the number and nature of the transformations separating it from its kernel. Descriptive sentences of varying grammatical form were presented to Ss who were asked to verify them, and the speed of verification was taken as an index of speed of understanding. Active sentences were found to be verified faster than passive, affirmative faster than negative, and true faster than false. The true-false variable was found to interact with the affirmative-negative, indicating that the latter difference is not simply syntactical. The consistency of the results with the hypothesis was noteworthy, but transformational complexity was confounded with frequency and length.},
author = {Gough, Philip B.},
doi = {10.1016/S0022-5371(65)80093-7},
file = {:home/tom/Documents/Mendeley Desktop/Gough - 1965 - Grammatical Transformations and Speed of Understanding.pdf:pdf},
issn = {00225371},
journal = {Journal of Verbal Learning and Verbal Behavior},
mendeley-groups = {Articles/Psychology,Negation paper},
number = {2},
pages = {107--111},
title = {{Grammatical Transformations and Speed of Understanding}},
url = {http://www.sciencedirect.com/science/article/pii/S0022537165800937{\%}5Cnhttp://linkinghub.elsevier.com/retrieve/pii/S0022537165800937},
volume = {4},
year = {1965}
}
@article{Grimmer2013,
abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Grimmer, Justin and Stewart, Brandon M},
doi = {10.1093/pan/mps028},
eprint = {9605103},
file = {:home/tom/Documents/Mendeley Desktop/Grimmer, Stewart - 2013 - Text as Data The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts.pdf:pdf},
isbn = {1047-1987},
issn = {10471987},
journal = {Political Analysis},
mendeley-groups = {Articles/PhD Proposal,Articles/PhD Proposal2,Articles/PhD,Articles/PhD/Text Analysis,Articles/Text Analysis,Articles/ECPR Interest Groups,Articles/MY459 Quantitative Text Analysis,PhD,Negation paper},
number = {3},
pages = {267--297},
primaryClass = {cs},
title = {{Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts}},
url = {http://pan.oxfordjournals.org/cgi/doi/10.1093/pan/mps028},
volume = {21},
year = {2013}
}
@book{Gutmann2004,
address = {Princeton},
author = {Gutmann, Amy and Thompson, Dennis F},
mendeley-groups = {Articles/PS - Deliberation,Negation paper},
publisher = {Princeton University Press},
title = {{Why Deliberative Democracy?}},
year = {2004}
}
@article{Holmes1973,
abstract = {An experiment was conducted to determine whether two-clause sentences are easier to perceive when the main clause occurs first than when the subordinate clause occurs first. Using a rapid visual processing task, this effect was obtained for adverbials and noun-phrase complements; but the reverse was found for relatives, that is, right-branching relatives were more difficult than center-embedded relatives. These results cannot be explained by a general principle postulating the primary role of the main clause in the perceptual organization of sentences. The results also indicate that singly center-embedded sentences are processed in a very different manner from multiply center-embedded sentences. {\textcopyright} 1973 Academic Press, Inc. All rights reserved.},
author = {Holmes, V M},
doi = {10.1016/S0022-5371(73)80072-6},
file = {:home/tom/Documents/Mendeley Desktop/Holmes - 1973 - Order of Main and Subordinate Clauses in Sentence Perception.pdf:pdf},
issn = {00225371},
journal = {Journal of Verbal Learning and Verbal Behavior},
mendeley-groups = {Articles/Psychology,Negation paper},
number = {3},
pages = {285--293},
title = {{Order of Main and Subordinate Clauses in Sentence Perception}},
volume = {12},
year = {1973}
}
@inproceedings{Honnibal2015,
author = {Honnibal, Matthew and Johnson, Mark},
booktitle = {EMNLP 2015},
file = {:home/tom/Documents/Mendeley Desktop/Honnibal, Johnson - 2015 - An Improved Non-monotonic Transition System for Dependency Parsing.pdf:pdf},
isbn = {9781941643327},
mendeley-groups = {Articles/NLP,Negation paper},
number = {September},
pages = {1373--1378},
title = {{An Improved Non-monotonic Transition System for Dependency Parsing}},
year = {2015}
}
@article{King2013,
abstract = {We offer the first large scale, multiple source analysis of the outcome of what may be the most extensive effort to selectively censor human expression ever implemented. To do this, we have devised a system to locate, download, and analyze the content of millions of social media posts originating from nearly 1,400 different social media services all over China before the Chinese government is able to find, evaluate, and censor (i.e., remove from the Internet) the large subset they deem objectionable. Using modern computer-assisted text analytic methods that we adapt and validate in the Chinese language, we compare the substantive content of posts censored to those not censored over time in each of 95 issue areas. Contrary to previous understandings, posts with negative, even vitriolic, criticism of the state, its leaders, and its policies are not more likely to be censored. Instead, we show that the censorship program is aimed at curtailing collection action by silencing comments that represent, reinforce, or spur social mobilization, regardless of content. Censorship is oriented toward attempting to forestall collective activities that are occurring now or may occur in the future and, as such, seem to clearly expose government intent, such as examples we offer where sharp increases in censorship presage government action outside the Internet.},
author = {King, Gary and Pan, Jennifer and Roberts, Margaret},
doi = {10.1017/S0003055413000014},
file = {:home/tom/Documents/Mendeley Desktop/King, Pan, Roberts - 2013 - How Censorship in China Allows Government Criticism but Silences Collective Expression.pdf:pdf},
isbn = {0003-0554},
issn = {0003-0554},
journal = {American Political Science Review},
mendeley-groups = {Articles,Articles/PS - Recent,Articles/Text Analysis,PhD,Negation paper},
number = {917},
pages = {326--343},
pmid = {7754995},
title = {{How Censorship in China Allows Government Criticism but Silences Collective Expression}},
url = {http://www.journals.cambridge.org/abstract{\_}S0003055413000014},
volume = {107},
year = {2013}
}
@article{Kluver2009,
abstract = {The analysis of interest group influence is crucial in order to explain policy outcomes and to assess the democratic legitimacy of the European Union. However, owing to methodological difficulties in operationalizing influence, only few have studied it. This article therefore proposes a new approach to the measurement of influence, drawing on quantitative text analysis. By comparing interest groups' policy positions with the final policy output, one can draw conclusions about the winners and losers of the decision-making process. In order to examine the applicability of text analysis, a case study is presented comparing hand-coding, WORDSCORES and Wordfish. The results correlate highly and text analysis proves to be a powerful tool to measure interest groups' policy positions, paving the way for the large-scale analysis of interest group influence.},
author = {Kl{\"{u}}ver, Heike},
doi = {10.1177/1465116509346782},
file = {:home/tom/Documents/Mendeley Desktop/Kl{\"{u}}ver - 2009 - Measuring Interest Group Influence Using Quantitative Text Analysis.pdf:pdf},
isbn = {1465-1165},
issn = {1465-1165},
journal = {European Union Politics},
mendeley-groups = {Articles/PhD/EU,Articles/PhD/Text Analysis,Articles/ECPR Interest Groups,PhD,Articles/Text Analysis,Negation paper},
number = {4},
pages = {535--549},
title = {{Measuring Interest Group Influence Using Quantitative Text Analysis}},
volume = {10},
year = {2009}
}
@book{Krippendorff2004,
abstract = {The Second Edition of Content Analysis: An Introduction to Its Methodology is a definitive sourcebook of the history and core principles of content analysis as well as an essential resource for present and future studies. The book introduces readers to ways of analyzing meaningful matter such as texts, images, voices - that is, data whose physical manifestations are secondary to the meanings that a particular population of people brings to them. Organized into three parts, the book examines the conceptual and methodological aspects of content analysis and also traces several paths through content analysis protocols. The author has completely revised and updated the Second Edition, integrating new information on computer-aided text analysis. The book also includes a practical guide that incorporates experiences in teaching and how to advise academic and commercial researchers. In addition, Krippendorff clarifies the epistemology and logic of content analysis as well as the methods for achieving its aims. Intended as a textbook for advanced undergraduate and graduate students across the social sciences, Content Analysis, Second Edition will also be a valuable resource for practitioners in a variety of disciplines.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krippendorff, Klaus},
doi = {10.2307/2288384},
edition = {2nd ed.},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/Mendeley Desktop/Krippendorff - 2004 - Content Analysis An Introduction to Its Methodology.pdf:pdf},
isbn = {0761915451},
issn = {01621459},
mendeley-groups = {Articles/Text Analysis,PhD,Negation paper},
pages = {440},
pmid = {370546962},
publisher = {Sage Publications},
title = {{Content Analysis: An Introduction to Its Methodology}},
year = {2004}
}
@book{Lasswell1949,
address = {Cambridge},
author = {Lasswell, Harold D and Leites, Nathan and Associates},
editor = {Lasswell, Harold D and Leites, Nathan and Associates},
file = {:home/tom/Documents/Mendeley Desktop/Lasswell, Leites, Associates - 1949 - Language of Politics Studies in Quantitative Semantics.pdf:pdf},
isbn = {026262009X},
mendeley-groups = {Articles/Text Analysis,PhD,Negation paper},
pmid = {65008781},
publisher = {MIT Press},
title = {{Language of Politics: Studies in Quantitative Semantics}},
year = {1949}
}
@article{Lau2009,
abstract = {The past two decades have seen an explosion of social science research on negative political advertising as the number of political observers complaining about its use—if not negative campaigning itself—has also grown dramatically. This article reviews the literature on negative campaigning—what candidates are most likely to attack their opponent, under what circumstances, and most importantly, to what effect. We also discuss the many serious methodological issues that make studying media effects of any kind so difficult, and make suggestions for “best practices” in conducting media research. Contrary to popular belief, there is little scientific evidence that attacking one's opponent is a particularly effective campaign technique, or that it has deleterious effects on our system of government. We conclude with a discussion of whether negative political advertising is bad for democracy.},
author = {Lau, Richard R and Rovner, Ivy Brown},
doi = {10.1146/annurev.polisci.10.071905.101448},
file = {:home/tom/Documents/Mendeley Desktop/Lau, Rovner - 2009 - Negative Campaigning.pdf:pdf},
issn = {1094-2939},
journal = {Annual Review of Political Science},
keywords = {attack advertising,demobilization,meta-analysis,negativity},
mendeley-groups = {Articles/PS - Parties,PhD,Negation paper},
pages = {285--306},
title = {{Negative Campaigning}},
volume = {12},
year = {2009}
}
@article{Lauderdale2016,
abstract = {Existing approaches to measuring political disagreement from text data perform poorly except when applied to narrowly selected texts discussing the same issues and written in the same style. We demonstrate the first viable approach for estimating legislator-specific scores from the entire speech corpus of a legislature, while also producing extensive information about the evolution of speech polarization and politically loaded language. In the Irish D{\'{a}} il, we show that the dominant dimension of speech variation is government–op-position, with ministers more extreme on this dimension than backbenchers, and a second dimension distinguishing between the establishment and anti-establishment opposition parties. In the U.S. Senate, we estimate a dimension that has moderate within-party correlations with scales based on roll-call votes and campaign donation patterns; however, we observe greater overlap across parties in speech positions than roll-call positions and partisan polarization in speeches varies more clearly in response to major political events.},
author = {Lauderdale, Benjamin E and Herzog, Alexander},
doi = {10.1093/pan/mpw017},
file = {:home/tom/Documents/Mendeley Desktop/Lauderdale, Herzog - 2016 - Measuring Political Positions from Legislative Speech.pdf:pdf},
issn = {14764989},
journal = {Political Analysis},
mendeley-groups = {Articles/Text Analysis,PhD,Negation paper},
number = {3},
pages = {374--394},
title = {{Measuring Political Positions from Legislative Speech}},
volume = {24},
year = {2016}
}
@article{Laver2003,
author = {Laver, Michael and Benoit, Kenneth and Garry, John},
file = {:home/tom/Documents/Mendeley Desktop/Laver, Benoit, Garry - 2003 - Extracting Policy Positions from Political Texts Using Words as Data.pdf:pdf},
journal = {American Political Science Review},
mendeley-groups = {Articles,Articles/MSc Thesis,Articles/PhD Proposal,Articles/PhD Proposal2,Articles/PhD,Articles/MY459 Quantitative Text Analysis,Articles/PhD/Text Analysis,Articles/Text Analysis,PhD,Negation paper},
number = {2},
pages = {311--331},
title = {{Extracting Policy Positions from Political Texts Using Words as Data}},
volume = {97},
year = {2003}
}
@article{Laver2000,
abstract = {The analysis of policy-based party competition will not make serious progress beyond the constraints of (a) the unitary actor assumption and (b) a static approach to analyzing party competition between elections until a method is available for deriving reliable and valid time-series estimates of the policy positions of large numbers of political actors. Retrospective estimation of these positions in past party systems will require a method for estimating policy positions from political texts. Previous hand-coding content analysis schemes deal with policy emphasis rather than policy positions. We propose a new hand-coding scheme for policy positions, together with a new English language computer-coding scheme that is compatible with this. We apply both schemes to party manifestos from Britain and Ireland in 1992 and 1997 and cross validate the resulting estimates with those derived from quite independent expert surveys and with previous manifesto analyses. There is a high degree of cross validation between coding methods, including computer coding. This implies that it is indeed possible to use computer-coded content analysis to derive reliable and valid estimates of policy positions from political texts. This will allow vast volumes of text to be coded, including texts generated by individuals and other internal party actors, allowing the empirical elaboration of dynamic rather than static models of party competition that move beyond the unitary actor assumption.},
author = {Laver, Michael and Garry, John},
doi = {10.2307/2669268},
file = {:home/tom/Documents/Mendeley Desktop/Laver, Garry - 2000 - Estimating Policy Positions from Political Texts.pdf:pdf},
isbn = {0092-5853},
issn = {00925853},
journal = {American Journal of Political Science},
mendeley-groups = {Articles/MY459 Quantitative Text Analysis,Articles/PhD/Text Analysis,Articles/Text Analysis,PhD,Negation paper},
number = {3},
pages = {619--634},
pmid = {446},
title = {{Estimating Policy Positions from Political Texts}},
url = {http://www.jstor.org.ezproxy.library.wisc.edu/stable/2669268{\%}5Cnhttp://www.jstor.org/stable/2669268},
volume = {44},
year = {2000}
}
@book{Lazarsfeld1948,
address = {New York},
author = {Lazarsfeld, Paul F and Berelson, Bernard and Gaudet, Hazel},
mendeley-groups = {Articles/Text Analysis,Negation paper},
publisher = {Columbia University Press},
title = {{The People's Choice: How the Voter Makes up his Mind in a Presidential Campaign}},
year = {1948}
}
@article{Lo2016,
abstract = {Parties in advanced democracies take ideological positions as part of electoral competition, but some parties communicate their position more clearly than others. Existing research on democratic party competition has paid much attention to assessing partisan position taking in electoral manifestos, but it has largely overlooked how manifestos reflect the clarity of these positions. This article presents a scaling procedure that better reflects the data-generating process of party manifestos. This new estimator allows us to recover not only positional estimates, but also estimates for the ideological clarity or ambiguity of parties. The study validates its results using Monte Carlo tests, a manifesto-drafting simulation and a human coding exercise. Finally, the article applies the estimator to party manifestos in four multiparty democracies and demonstrates that ambiguity can enhance the appeal of parties with platforms that become more moderate, and lessen the appeal of parties with platforms that become more extreme.},
author = {Lo, James and Proksch, Sven-Oliver and Slapin, Jonathan B},
file = {:home/tom/Documents/Mendeley Desktop/Lo, Proksch, Slapin - 2016 - Ideological Clarity in Multiparty Competition A New Measure and Test Using Election Manifestos.pdf:pdf},
journal = {British Journal of Political Science},
mendeley-groups = {Articles/PS - Parties,Articles/Text Analysis,Negation paper},
number = {3},
pages = {591--610},
title = {{Ideological Clarity in Multiparty Competition: A New Measure and Test Using Election Manifestos}},
volume = {46},
year = {2016}
}
@unpublished{Lowe2013a,
abstract = {--},
author = {Lowe, Will},
booktitle = {SSRN},
file = {:home/tom/Documents/Mendeley Desktop/Lowe - 2013 - There's (basically) only one way to do. Working Paper.pdf:pdf},
mendeley-groups = {Articles/PhD,Articles/PhD/Text Analysis,PhD,Articles/Text Analysis,Negation paper},
pages = {1--17},
title = {{There's (basically) only one way to do. Working Paper}},
year = {2013}
}
@article{Mehler1963,
abstract = {Summary$\backslash$nThe recall of English sentences varying systematically in syntactic structure was studied by the method of prompted recall with 80 Ss. Analysis of the errors indicated that most of them were due to syntactical confusions. The hypothesis is advanced that Ss analyze the sentences into a semantic component plus syntactic corrections when they learn them, and that this separation of semantic content from syntactic form is one reason that the general meaning of a message is generally so much easier to recall than its exact wording.},
author = {Mehler, Jacques},
doi = {10.1016/S0022-5371(63)80103-6},
file = {:home/tom/Documents/Mendeley Desktop/Mehler - 1963 - Some Effects of Grammatical Transformations on the Recall of English Sentences.pdf:pdf},
isbn = {0022-5371},
issn = {00225371},
journal = {Journal of Verbal Learning and Verbal Behavior},
mendeley-groups = {Articles/Psychology,Negation paper},
number = {4},
pages = {346--351},
title = {{Some Effects of Grammatical Transformations on the Recall of English Sentences}},
volume = {2},
year = {1963}
}
@article{Mikhaylov2012,
abstract = {The Comparative Manifesto Project (CMP) provides the only time series of estimated party policy positions in political science and has been extensively used in a wide variety of applications. Recent work (e.g., Benoit, Laver, and Mikhaylov 2009; Klingemann et al. 2006) focuses on nonsystematic sources of error in these estimates that arise from the text generation process. Our concern here, by contrast, is with error that arises during the text coding process since nearly all manifestos are coded only once by a single coder. First, we discuss reliability and misclassification in the context of hand-coded content analysis methods. Second, we report results of a coding experiment that used trained human coders to code sample manifestos provided by the CMP, allowing us to estimate the reliability of both coders and coding categories. Third, we compare our test codings to the published CMP "gold standard" codings of the test documents to assess accuracy and produce empirical estimates of a misclassification matrix for each coding category. Finally, we demonstrate the effect of coding misclassification on the CMP's most widely used index, its left-right scale. Our findings indicate that misclassification is a serious and systemic problem with the current CMP data set and coding process, suggesting the CMP scheme should be significantly simplified to address reliability issues.},
author = {Mikhaylov, Slava and Laver, Michael and Benoit, Kenneth},
file = {:home/tom/Documents/Mendeley Desktop/Mikhaylov, Laver, Benoit - 2012 - Coder Reliability and Misclassification in the Human Coding of Party Manifestos.pdf:pdf},
journal = {Political Analysis},
mendeley-groups = {Articles,Articles/PhD/Text Analysis,PhD,Articles/Text Analysis,Negation paper},
number = {1},
pages = {78--91},
title = {{Coder Reliability and Misclassification in the Human Coding of Party Manifestos}},
volume = {20},
year = {2012}
}
@article{Miller1962,
abstract = {Language is "an extremely complicated human skill." What it consists of and how it functions requires detailed examination. Some satisfactory way of dealing with grammar and its combinatorial processes is required to describe language as a skill. Concepts of modern linguistics are used. Psychological aspects of syntactic structure are considered and specified as important variables to explore. Linguistic theory and empirical studies are cited. "I believe that one of the best ways to study a human mind is by studying the verbal systems that it uses." Such a program is not only important, but immediately possible. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Miller, George A},
doi = {10.1037/h0044708},
file = {:home/tom/Documents/Mendeley Desktop/Miller - 1962 - Some psychological studies of grammar.pdf:pdf},
issn = {0003-066X},
journal = {American Psychologist},
mendeley-groups = {Articles/Psychology,Negation paper},
number = {11},
pages = {748--762},
title = {{Some psychological studies of grammar}},
volume = {17},
year = {1962}
}
@article{Roberts2014,
abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, andAiroldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STMis helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
author = {Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G},
doi = {10.1111/ajps.12103},
file = {:home/tom/Documents/Mendeley Desktop/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Responses(2).pdf:pdf;:home/tom/Documents/Mendeley Desktop/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Responses(3).pdf:pdf},
isbn = {0092-5853},
issn = {15405907},
journal = {American Journal of Political Science},
mendeley-groups = {Articles/Text Analysis,Articles/Text Analysis/Topic Modelling,PhD,Negation paper},
number = {4},
pages = {1064--1082},
title = {{Structural Topic Models for Open-Ended Survey Responses}},
volume = {58},
year = {2014}
}
@article{Slapin2008,
author = {Slapin, Jonathan B and Proksch, Sven-Oliver},
file = {:home/tom/Documents/Mendeley Desktop/Slapin, Proksch - 2008 - A Scaling Model for Estimating Time-Series Party Positions from Texts.pdf:pdf},
journal = {American Journal of Political Science},
mendeley-groups = {Articles,Articles/PhD Proposal,Articles/PhD Proposal2,Articles/PhD,Articles/MY459 Quantitative Text Analysis,Articles/PhD/Text Analysis,Articles/Text Analysis,PhD,Negation paper},
number = {3},
pages = {705--722},
title = {{A Scaling Model for Estimating Time-Series Party Positions from Texts}},
volume = {52},
year = {2008}
}
@article{Spirling2015,
abstract = {We consider the impact of the Second Reform Act, and the doubling of the electorate it delivered, on the linguistic complexity of speeches made by members of parliament in Britain. Noting that the new voters were generally poorer and less educated than those who already enjoyed the suffrage, we hypothesize that cabinet ministers had strong incentives—relative to other members—to appeal to these new electors with simpler statements during parliamentary debates. We assess this claim with a data set of over half a million speeches for the period between the Great Reform Act and Great War, along with methods for measuring the comprehensibility of texts—which we validate in some detail. The theorized relationship holds: ministers become statistically significantly easier to understand (on average) relative to backbenchers, and this effect occurs almost immediately after the 1868 election. We show that this result is not an artifact of new personnel in the House of Commons. F},
author = {Spirling, Arthur},
doi = {10.1525/jsah.2010.69.3.430.display},
file = {:home/tom/Documents/Mendeley Desktop/Spirling - 2015 - Democratization and Linguistic Complexity The Effect of Franchise Extension on Parliamentary Discourse, 1832-1915.pdf:pdf},
isbn = {1857880560},
issn = {0022-3816},
journal = {The Journal of Politics},
mendeley-groups = {Articles/PS - Elections,Articles/Text Analysis,PhD,Negation paper},
number = {1},
pages = {235--248},
title = {{Democratization and Linguistic Complexity: The Effect of Franchise Extension on Parliamentary Discourse, 1832-1915}},
volume = {78},
year = {2015}
}
@article{Steenbergen2003,
abstract = {In this paper, we develop a discourse quality index (DQI) that serves as a quantitative measure of discourse in deliberation. The DQI is rooted in Habermas' discourse ethics and provides an accurate representation of the most important principles underlying deliberation. At the same time, the DQI can be shown to be a reliable measurement instrument due to its focus on observable behavior and its detailed coding instructions. We illustrate the DQI for a parliamentary debate in the British House of Commons. We show that the DQI yields reliable data and we discuss how these data could be used in subsequent analysis. We conclude by discussing some limitations of the DQI and by identifying some areas in which it could prove useful.},
author = {Steenbergen, Marco R and B{\"{a}}chtiger, Andr{\'{e}} and Sp{\"{o}}rndli, Markus and Steiner, J{\"{u}}rg},
doi = {10.1057/palgrave.cep.6110002},
file = {:home/tom/Documents/Mendeley Desktop/Steenbergen et al. - 2003 - Measuring Political Deliberation A Discourse Quality Index.pdf:pdf},
isbn = {doi:10.1057/palgrave.cep.6110002},
issn = {14724790},
journal = {Comparative European Politics},
keywords = {deliberation,discourse,measurement,parliamentary debate},
mendeley-groups = {Articles/Denisa project,Articles/PS - Deliberation,Negation paper},
number = {1},
pages = {21--48},
title = {{Measuring Political Deliberation: A Discourse Quality Index}},
volume = {1},
year = {2003}
}
@unpublished{Stegmueller2017,
author = {Stegmueller, Daniel},
file = {:home/tom/Documents/Mendeley Desktop/Stegmueller - 2017 - Individual heterogeneity in survey response behavior. A Bayesian random coefficient IRT model .pdf:pdf},
mendeley-groups = {Articles/PS - Methodology/Survey,Negation paper},
pages = {1--26},
title = {{Individual heterogeneity in survey response behavior. A Bayesian random coefficient IRT model .}},
year = {2017}
}
@article{Takane1987,
author = {Takane, Yoshio and {De Leeuw}, Jan},
file = {:home/tom/Documents/Mendeley Desktop/Takane, De Leeuw - 1987 - On the relationhip between item response theory and factor analysis of discretized variables.pdf:pdf},
journal = {Psychometrika},
keywords = {categorical data,dichotomous data,marginal maximum likelihood estimation,ordered and unordered,pair comparison data},
mendeley-groups = {Articles/Psychology - Methodology,Negation paper},
number = {3},
pages = {393--408},
title = {{On the relationhip between item response theory and factor analysis of discretized variables}},
url = {http://takane.brinkster.net/Yoshio/p026.pdf},
volume = {52},
year = {1987}
}
@book{Townsend2001,
address = {Cambridge, Massachussets},
author = {Townsend, Thomas G and Bever, David J},
file = {:home/tom/Documents/Mendeley Desktop/Townsend, Bever - 2001 - Sentence Comprehension.pdf:pdf},
isbn = {0-262-20132-1},
mendeley-groups = {Negation paper},
publisher = {Massachusetts Intitute of Technology},
title = {{Sentence Comprehension}},
url = {http://sfx.ucl.ac.uk/sfx{\_}local?ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8{\&}ctx{\_}tim=2017-08-30T16{\%}3A08{\%}3A01IST{\&}url{\_}ver=Z39.88-2004{\&}url{\_}ctx{\_}fmt=infofi{\%}2Ffmt{\%}3Akev{\%}3Amtx{\%}3Actx{\&}rfr{\_}id=info{\%}3Asid{\%}2Fprimo.exlibrisgroup.com{\%}3Aprimo3-Journal-UCL{\_}LMS{\_}DS{\&}r},
year = {2001}
}
@book{Volkens2013,
abstract = {The Manifesto data are the only comprehensive set of policy indicators for social, economic and political research. It is thus vital that their quality is established. The purpose of this book is to review methodological issues that have got in the way of straightforwardly using the Manifesto data since our two preceding volumes were published and to resolve them in ways which best serve users and textual analysts in general. The book is thus generally about text-based quantitative analysis with a particular focus on the quality of the CMP-MARPOR data and ways of assessing and using them, In doing so the book goes beyond normal data documentation - essential though that is - to confront the analytic issues faced by users of the data now distributed by MARPOR. It also provides concrete strategies for tackling these at the research level, with examples from the field of political representation. The problems of uncertainty, error, reliability and validity considered here are generic issues for political analysts in any area of research, so the book has an interest extending beyond the Manifesto estimates themselves - in particular to other textual analyses. In addition the book widens the range of applications introduced in our two previous volumes and discusses the extension of the manifesto project database to cover Latin America.},
address = {Oxford},
author = {Volkens, Andrea and Bara, Judith and Budge, Ian and McDonald, Michael D. and Klingemann, Hans-Dieter},
file = {:home/tom/Documents/Mendeley Desktop/Volkens et al. - 2013 - Mapping Policy Preferences from Texts III Statistical Solutions for Manifesto Analysts.pdf:pdf},
isbn = {9780199640041},
mendeley-groups = {Articles/PhD/Text Analysis,PhD,Articles/Text Analysis,Negation paper},
pages = {346},
publisher = {Oxford University Press},
title = {{Mapping Policy Preferences from Texts III: Statistical Solutions for Manifesto Analysts}},
url = {https://books.google.at/books?id=lWDSAQAAQBAJ},
year = {2013}
}




@Misc{VLM+2016,
  author    = {Volkens, Andrea and Lehmann, Pola and Matthieß, Theres and Merz, Nicolas and Regel, Sven},
  title     = {Manifesto Project Dataset (version 2016b)},
  year      = {2016},
  owner     = {CM},
  timestamp = {2018-01-22},
}

@Article{MRL2016-04,
  year         = {2016},
  date         = {2016-04},
  journal      = {Research \& Politics},
  title        = {The Manifesto Corpus: A new resource for research on political parties and quantitative text analysis},
  author       = {Nicolas Merz and Sven Regel and Jirka Lewandowski},
  number       = {2},
  pages        = {1--8},
  volume       = {3},
  month        = {4},
  doi          = {10.1177/2053168016643346},
}

@article{Jackman2001,
	author = {Simon Jackman},
	year = {2001},
	doi = {10.1093/polana/9.3.227},
	journal = {Political Analysis},
	number = {3},
	pages = {227–241},
	title = {Multidimensional Analysis of Roll Call Data via Bayesian Simulation: Identification, Estimation, Inference, and Model Checking},
	volume = {9}
}

@book{DBC+2014,
	author = {Armstrong, II, David A. and Ryan Bakker and Royce Carroll and Christopher Hare and Keith T. Poole and Howard Rosenthal},
	year = {2014},
	keywords = {computation, R},
	location = {Boca Raton, FL},
	publisher = {CRC Press},
	title = {Analyzing Spatial Models of Choice and Judgement with {R}}
}

@article{CGH+2016,
	author = {Bob Carpenter and Andrew Gelman and Matt Hoffman and Daniel Lee and Ben Goodrich and Michael Betancourt and Michael A. Brubaker and Jiqiang Guo and Peter Li and Allen Riddell},
	year = {2017},
  volume = {71},
  number = {1},
  pages = {1--32},
	journal = {Journal of Statistical Software},
	title = {Stan: A probabilistic programming language},
  doi = {10.18637/jss.v076.i01},
}

@Misc{RStan2018,
  title = {{RStan}: the {R} interface to {Stan}},
  author = {{Stan Development Team}},
  note = {R package version 2.17.3},
  year = {2018},
  url = {http://mc-stan.org/},
}

@Misc{,
  title = {{RStan}: the {R} interface to {Stan}},
  shortauthor = {{Stan}},
  author = {{Stan Development Team}},
  note = {R package version 2.17.3},
  year = {2018},
  url = {http://mc-stan.org/},
}

@Manual{R2017,
	title = {R: A Language and Environment for Statistical Computing},
	author = {{R Core Team}},
	organization = {R Foundation for Statistical Computing},
	address = {Vienna, Austria},
	year = {2017},
	url = {https://www.r-project.org},
}

@article{GS2013,
	author = {Justin Grimmer and Brandon M. Stewart},
	year = {2013},
	date = {2013},
	doi = {10.1093/pan/mps028},
	journal = {Political Analysis},
	number = {3},
	pages = {267–297},
	timestamp = {2013-07-18},
	title = {Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts},
	volume = {21}
}

@TechReport{CP2012,
  author      = {Jinho D. Choi and Martha Palmer},
  title       = {Guidelines for the {CLEAR} Style Constituent to Dependency Conversion},
  institution = {Institute of Cognitive Science, University of Colorado Boulder},
  year        = {2012},
  number      = {01-12},
  address     = {Boulder, CO},
  owner       = {CM},
  timestamp   = {2018-01-28},
}

@InProceedings{CTS2015,
  author    = {Jinho D. Choi and Joel Tetreault and Amanda Stent},
  title     = {It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
  year      = {2015},
  pages     = {387--396},
  publisher = {Association for Computational Linguistics},
  owner     = {CM},
  timestamp = {2018-01-28},
}